---
title: 'NICAR 2020 - R1: Intro to R and RStudio'
author: "Meghan Hoyer & Ryan Thornburg"
date: "3/7/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Welcome to R

R is a powerful open-source programming and statistical language. Scientists, statisticians and, increasingly, journalists are using it to find answers in mountains of data. People turn to R for everything from analyzing multi-year medical studies to preparing charts and drawing maps. The language itself, known as “base R”, can do quite a bit. But computer scientists have vastly expanded its capabilities with more than 12,000 add-on tools or packages.

R is going to help you do more complex analyses in an easier way. But there is something of a learning curve. For one, R is a strict copy-editor - capitalization and spelling matter, so make sure you stick to some naming conventions and spell things correctly. Beyond that, and the biggest issue is there are approximately 1,543 ways to solve any problem in R. I'd recommend hewing as closely as you can to the principles of "tidy" data as you work. 

The most popular of R packages is a suite of about 20 tools known as the tidyverse. It was developed largely by Hadley Wickham, chief scientist at RStudio, the graphical user interface for R. The tidyverse includes tools for importing, manipulating, editing and visualizing data and is based around a basic principle: that data be tidy, with one observation, or data point, in each row.
"When your data is tidy, each column is a variable, and each row is an observation."

If you're looking for more resources or next steps to help you learn, [here](https://github.com/roncampbell/NICAR2019/blob/master/R%20reading%20list.md) is a pretty comprehensive list of RStudio + R resources, including books, tipsheets and webinars compiled by Ron Campbell and me. 

### First up: The basics of RStudio

RStudio is pretty nifty because it lets you see all the pieces of your project, and it even saves you from mistakes or problems later on by allowing you to run all or part of your code, and by keeping your history and showing you your loaded dataframes, functions and lists.

Here's a primer:

* The CONSOLE (bottom left) is where you can type in commands

* The SOURCE (top left) is where you'll see/open your scripts and also View() any datatable. Datatables opened in source (either by double-clicking from Environment or calling View() in your script or console) can also be explored through sorting and filtering

* ENVIRONMENT (top right) is your loaded data

* HISTORY (top right) is every command you've run, starting with your most recent command

* FILES (bottom left) allows you to navigate through your directory

* PLOTS (bottom left) shows you any output from ggplot

* PACKAGES (bottom left) shows you your system library, your package versions, and allows you to install new packages if needed

* HELP (bottom left) includes documentation for packages

### Setting Up Your Work
Since R is open-source, it is free. You can download R and most packages at CRAN, the Comprehensive R Archive Network. Installing and loading packages is a two-step process. To install the tidyverse, for example, you would enter the following commands at the console - note the quotes around the package name:

```{r tidyverse}
#install.packages("tidyverse")
```

IRE-NICAR staff has already installed R, R Studio and the tidyverse on the computers we’ll be using in this class. But whenever you start R, you must do step 2: loading the packages you need in memory. All library calls should be included at the top of your scripts; that way, anyone using your code can see and load all the necessary packages to run your code.

So - let's open a new script (File -> New File -> R Script) and load the tidyverse package so we can get to work. Note for library calls there are no quotes around the library name.

```{r library}
library(tidyverse)
```

Generally, when you work in R, it's best to create a 'Project' in RStudio to make sure your installed packages, files and all other pieces are in one place. While we aren’t creating a project for this class, we will go partway – we’ll set a working directory. The directory we’ll use is wherever the IRE-NICAR staff placed our data. To set the working directory go to the Session menu and click on Set Working Directory. We’ll then look for the correct directory. Best practice: never code a hard-path to setwd() into your script - it'll work on your machine, but won't on anyone else's.

```{r getwd}
getwd()
#setwd()
```

### R Syntax

There are many parts of R that look a lot like other data tools -- functions such as filtering, summing, and the like. But there are two main tools in writing R code that may be less familiar to you. First, to assign a value to a variable, we use an arrow-like thing called an assignment operator. 

Here it is: <- 

We’ll also be writing multi-line formulas, joining each line with a device called a pipe. It looks like this: %>%. There’s a shortcut for this: In Windows, type Shift-Control-M; in Mac, type Shift-Command-M.  The pipe always goes at the end of a line, followed by a return.

Beyond that -- give future you and anyone else who will use or read your code a break: Good R practice is to put spaces around assignment operators, to do a hard return after a pipe, and stick to a naming convention for dataframes, functions and variables. There are some styleguides for R [here](https://style.tidyverse.org/_main.pdf) and [here](https://google.github.io/styleguide/Rguide.xml) that will help you think about best practices.

Also don't forget to comment out your code! You can add a comment by putting in an R Script by putting a '#' before your words.

##Reading in Data

Finally -- let's dig in! Today we’re going to be creating several tables, known in R as data frames, and naming them as we go. 

Here's the [file](https://tinyurl.com/nicar19rstudiodata) you'll need. Download it and put it in the folder for this class that you've set as your directory above.

Now let's use read_csv to pull data from the Census into R.

There are other options for reading in files that are tab or pipe-delimited, in .txt, .xlsx or other formats -- including SAS and Stata files, tables from a SQL database, Google sheets or the like. Note that for some of these, you'll need to download additional libraries. 

```{r read_in_data}
census <- read_csv("data/grandparentsR1.csv")

```

R does a pretty decent job of guessing your data types, but every once in awhile it messes them up and you'll have to rework them. For instance, check that id2 value. You can declare specific data types for each column while using read_csv, or you can change types on individual columns. For this, let's just fix the column individually by doing:

```{r rework}
census$id2 <- as.character(census$id2)
```

### Looking at your Data
Now, let's open this dataframe up and explore as we might explore an Excel spreadsheet.

In your console, type:

```{r viewing}
View(census)
```

To limit the View on a large file you can also call head with a certain number of rows, similar to LIMIT in SQL:

```{r view2}
head(census, 5) %>% View()
```

You can sort and filter this View, which allows you to get a basic familiarity with your dataframe. 

But now let's start really digging into this to get an idea of what might be in our data. Summary -- which is part of base R -- isn't a bad place to start looking at things, particularly if you have a lot of numbers, because it returns minimum and maximum values for each column, as well as averages and medians. 

```{r summary}
summary(census)
```

### Tidying your data 

OK, that's great and all, but it's still really hard to understand what we're looking at. This data isn't tidy! There are lots of different demographic group breakdowns per row -- we don't have just one observation in each row.

Let's tidy things up using a function called "pivot_longer" (which has recently replaced a similar function called "gather" that you will still see in tutorials that are more than about a year old.) 

This is a somewhat complex operation, so you don't need to totally understand how it works right now, but basically we're reshaping our data, so that the basic information (FIPS codes, city name and total population) are attached to each individual observation (the breakdowns of the type of population and the count of that group).  We're creating an individual row based on columns 5 - 9 of our dataframe. 

Where data in Excel is usually wide, this makes our data long. The opposite of gather is spread, and it can work to make tables wide, with multiple observations per row if you so desire.

```{r tidydata}
grandparents <- census %>% pivot_longer(cols = (5:9), names_to ="type", values_to = "pop_in_category")

```

Open that file up with View(grandparents). You'll see that now your datafram has a LOT more rows -- 320 of them to be exact -- but a fewer columns (only five). Now that things are tidy, we can really start to explore.

### Sanity-checking

You want to make sure the data is OK and you don't have duplicate parishes.

```{r count1}
census %>% distinct(Geography) %>% count()
```

You can also use count to see how many of a certain thing meets your filter criteria. Here, let's see how many parishes have more than 100,000 people:

```{r count2}
census %>% filter(TotalPop>100000) %>% distinct(Geography) %>% count()
```
### Filtering and arranging your data

You're probably familiar with filtering from Excel, or from using "WHERE" statements in SQL. This is similar.

 Note that filtering on strings and characters requires a double equal sign.

```{r filtering}
census %>% 
  filter(Geography == "West Feliciana Parish, Louisiana") 
```


### Removing and adding columns: Select & Mutate functions

While the filter() function in R allows you to see only certain rows, the select() function lets you see only certain columns.

```{r selecting}
census %>% 
  filter(Geography == "West Feliciana Parish, Louisiana")  %>%
  select(Geography, TotalPop, PopOver30, Resp4GrandChildren)

```

In R, creating a new column in your data is called _Mutating_ -- basically, you're changing your dataframe by adding a new column without changing anything in your existing dataframe. 

```{r mutating}
census %>% 
  mutate(pct = Resp4GrandChildren/PopOver30)

```

Sometimes its useful to create a new dataframe from once you've added and removed the columns you want for your final analysis. Here we'll create a dataframe called census_focused.

```{r}
census_focused <- census %>% 
  mutate(pct = Resp4GrandChildren/PopOver30) %>%
  select(Geography, TotalPop, PopOver30, Resp4GrandChildren, pct)
```


And since we don't want to have to go in and sort every time, so let's make it so the greatest share of grandparents caring for grandchildren is on top.

```{r filterarrange}
census_focused %>% 
  arrange(desc(pct)) 
```

## Summarize

Inside the summarize (or summarise) function there are many basic math operators -- including mean, median, minimum, maximum and sum. For all of Louisiana's 64 parishes, let's find out what's the minimum, maximum and median percent of grandparents who are raising their grandchildren.

```{r summarise}
census_focused  %>% 
  summarise(Median = median(pct), Minimum = min(pct), Maximum = max(pct))
```


## Creating and comparing groups

So we're working on a story and only want to look at parishes around New Orleans. And then maybe compare the New Orleans region to the rest of the state.

How do we do that?

First we'll need to filter to isolate the right parishes. Then we'll need to tell R to then add up the total population and the care-giving grandparents for all the parishes in each group.

There's a lot going on here, so let's break it down:

First, the filter.  If you want to combine any group of things in R, you put parentheses around them and delineate them with a 'c' -- so here, we're saying let's filter by choosing these two types of populations. 

Rather than writing _filter(Geography == "Orleans Parish, Louisiana", Geography == "St. Bernard Parish, Louisiana", Geography == "St. Tammany Parish, Louisiana", Geography == "Jefferson Parish, Louisiana", Geography == "Plaquemines Parish, Louisiana")_ we're going to create a new variable, which will be the set of parishes in the New Orleans region.

```{r}
NOLA_region <- c("Orleans Parish, Louisiana", "St. Bernard Parish, Louisiana", "St. Tammany Parish, Louisiana", "Jefferson Parish, Louisiana", "Plaquemines Parish, Louisiana")
```


Next: Once we've created this collection of parishes that we're calling "NOLA_region" we need to create a new column and store in that column information about whether the parish is in the New Orleans region or in the rest of the state. We can create these new categorical variables using the case_when() function. 

In the case_when function you will see the _~_, which can be read as "then".

You will also see a new operator called _%in%_ . We use this when we want to ask if a value is equal to any one of many values. And we also see the _!_ operator. You may know that this means "not", but in R the _!_ operator can sometimes go in places you're not accustomed to seeing it.

So this can be read as: 

1. "Start with the census dataframe ...
2. "and create a new column called region ...
3. "and insert "NOLA Region" as the value of the new region column anytime the value of the Geography column is in the set of values in NOLA_Region. ...
4. "and if the value isn't in the set, then set the value of the region column to "Rest of State" ...
5. "and then take each row and put it into groups based on the values in the region column ... "
6. "and then, for each group, add the values of PopOver30 and Resp4GrandChildren. Put those summary values into new columns called RegionalPopOver30 and RegionalResp4GrandChildren ...
7. "and then divide RegionalResp4GrandChildren by RegionalPopOver30 and save that result in yet another new column called regional_pct"

#### The code:
```{r exercise}
regional <- census %>%
    mutate(region = 
             case_when(Geography %in% NOLA_region ~ "NOLA Region", !(Geography %in% NOLA_region) ~ "Rest of State") ) %>%
  group_by(region) %>%
  summarize(RegionalPopOver30 = sum(PopOver30), RegionalResp4GrandChildren = sum(Resp4GrandChildren)) %>%
  mutate(regional_pct = RegionalResp4GrandChildren / RegionalPopOver30)

```


